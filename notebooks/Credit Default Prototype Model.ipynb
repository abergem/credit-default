{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREDIT DEFAULT PROTOTYPE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Adrian Bergem <br>\n",
    "Date: 31/10/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook is a prototype for a credit default model for client xyz. <br>\n",
    "\n",
    "The are 3 <b>datasets</b> available, namely borrower, loan and payments. All three datasets have 887 379 rows and 14, 18 and 41 columns respectively. As the payment data seems to be dated recently, I argue it cannot be used in a model where one is trying to predict credit default straight after receiving their loan approval. I also omit data from loans that were originated prior to 2010 to avoid market dynamics from the financial crisis. \n",
    "\n",
    "Loan status is used to construct the <b>target variables</b>. I find it reasonable to only include the loans that are terminated – i.e. the ones with a status as either 'Charged Off' or 'Fully Paid' - in order to compare apples to apples. The stratified train-test split shows a class imbalance where positives (defaults) make up 18% of the train data.  I have chosen not to oversample/undersample the data due its shortcomings (loss of training data or reusage of data causing bias) compared to the relatively low imbalance.\n",
    "\n",
    "The <b>EDA</b> is quite extensive as I go through all borrower and loan features in detail, evaluating their usefulness. The features that seem to be most important (verified by the model’s features importance) are address state (US state provided by the of the borrower’s address), sub-grades (granular grade assigned by the client) and annual income of the borrower.\n",
    "\n",
    "The <b>data pre-processing</b> consists of cleaning some of the features, dropping the ones with (almost) no variance, dropping those that correlate >99% with other features, dropping the ones with >90% NaNs, and those that require too much feature engineering to make use of. Some string features such as 'Description' and 'Employment Title' could be worth looking more into with more feature engineering.\n",
    "\n",
    "As <b>scoring metric</b>, I'm using roc-auc as in this case one might argue that predicting both classes is important, and hence focusing on both TPR and FPR. I would think a company with a business model as the intermediate between borrowers and lenders would benefit from a decent FPR to not forego too much business opportunities.\n",
    "\n",
    "The <b>modeling</b> is done by first spot-checking 3 different models, namely logistic regression with the SGD algorithm, sklearn's Random Forest and LightGBM. Showing a consistenly higher auc (above 0.70), I progress with the LightGBM to find the best hyperparameters using Bayesian Optimization (with the Hypopt library). Due to its requirement for computational power, I use a sample of the training data to perform the kfold cv iterations to find optimal hyperparameters. Using these optimal hyperparameters on a kfold cv on the full training set actually shows a slightly lower auc score than with the spot-checked parameters (but still above 0.70). This is likely due to the sampling of the training data. On the bright side, the optimization shows a positive trend in score with respect to the number of iterations, and would likely lead to a higher score when using the full training set for optimization. When developing the model for production, I would thus suggest to use bayesian optimization for the hyperparameters, but training it on the full training set.\n",
    "\n",
    "The <b>results on the test set</b> are good, with a roc-auc of ~0.71 – showing no signs of overfitting. The model makes it easy to experiment with different thresholds for class prediction, and hence work with the TPR/FPR trade-off. I think this should be done along with the client to agree on which metrics are most important. \n",
    "\n",
    "To assess the model's <b>business value</b>, I compare it to a model that might be similar to the one being used today, i.e. a model based on sub-grades. The subgrade model is built by allocating a probability of default linearly across the sub-grades (from 0% on A1 to 50% on G5). By tweeking the threshold of the subgrade model to match the ML model's number of TPs, one can compare the performance by looking at the confusion matrices. The ML model clearly outperforms the subgrade model with 201 less FNs and 1307 less FPs, clearly demonstrating the ML model’s added business value. \n",
    "\n",
    "The <b>limitation</b> of the model, as briefly mentioned, is that I have not treated the classes imbalance. Although the imbalance isn't as bad as many other applications, one could possibly obtain improvements by using sklearn's RandomUnderSampler() and LightGBM's scale_pos_weight parameter. I am though not too comfortable with those techniques, which is the reason why I haven't tried them out here. For the next stage, when working with a model to be deployed into production, I would suggest to try out these techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Libs imports and settings <br>\n",
    "<br>\n",
    "2. Import data and quick inspection <br>\n",
    "<br>\n",
    "3. Setting up the problem <br>\n",
    "    3.1 Merge dataset <br>\n",
    "    3.2 Overview of credit default <br>\n",
    "    3.3 Set up the target variable <br>\n",
    "    3.4 Preliminary data cleaning: remove low-value features <br>\n",
    "    3.5 Omit old data <br>\n",
    "    3.6 Split train and test data <br>\n",
    "<br>\n",
    "4. EDA <br>\n",
    "    4.1 Functions <br>\n",
    "    4.2 Data on borrower/demographics <br>\n",
    "    4.3 Loan data <br>\n",
    "    4.4 Feature engineering <br>\n",
    "<br>\n",
    "5. Data pre-processing <br>\n",
    "<br>\n",
    "6. Modeling <br>\n",
    "    6.1 Spot-check 3 different algorithms using grid search cross-validation <br>\n",
    "    6.2 Bayesian Hyperparameter Optimization <br>\n",
    "    6.3 Results of best model on full train data <br>\n",
    "    6.4 Feature importances with best model <br>\n",
    "<br>\n",
    "7. Results on test data <br>\n",
    "    7.1 Pre-process test data <br>\n",
    "    7.2 Confusion matrix <br>\n",
    "    7.3 ROC curve and ROC-AUC score <br>\n",
    "    7.4 Comparison with sub-grade model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libs imports and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Imports:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File system management\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "\n",
    "# numpy and pandas for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# datetime libs\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# matplotlib, seaborn and wordcloud for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# itertools for efficient looping\n",
    "import itertools\n",
    "\n",
    "# Wordcloud for exploring string features\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "# scikit-learn for modeling\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# LightGBM for gradient boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Hyperopt for bayesian hyperparameter optimization\n",
    "#from hyperopt import hp\n",
    "#from hyperopt.pyll.stochastic import sample\n",
    "#from hyperopt import tpe\n",
    "#from hyperopt import Trials\n",
    "#from hyperopt import fmin\n",
    "#from hyperopt import STATUS_OK\n",
    "#import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Settings:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print settings\n",
    "from pprint import pprint\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(r'C:\\Users\\adrian.bergem\\Google Drive\\Data science\\Projects\\AI Credit Default')\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global modeling settings\n",
    "seed = 42\n",
    "\n",
    "# Global plotting settings\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data and quick inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the 3 various csv files (the payment csv was created from the .rda R file using the following R code: <br>\n",
    "payments <- load(\"\\\\Loan Data\\\\Loan Payment Information.Rda\")\n",
    "write.csv(Payment.df, file = \"Loan_payment_information.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower = pd.read_csv(r'data\\raw\\Borrower Information.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan = pd.read_csv(r'data\\raw\\Loan Classification Information.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment = pd.read_csv('Loan Data\\\\Loan_payment_information.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will inspect the data to get an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Borrower dataset shape:', borrower.shape)\n",
    "print('Loan dataset shape:', loan.shape)\n",
    "print('Payment dataset shape:', payment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "borrower.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payment.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting up the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important question is what the time stamp of the data we have at hand is. If we are to train a model on historical data, we need to be careful not to include forward-looking data.\n",
    "\n",
    "One needs to make the assumption that loan and demographics data is from the time of the loan origination. The payments data seems to be recent data, and can thus not be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique customers per loan\n",
    "loan['member_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be only 1 loan per borrower, hence there is a 1:1 relationship between the borrower and the loan dataset. We can thus merge the 2 datasets on the member_id key (primary key in borrower and foreign key in loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(borrower, loan, on='member_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887379, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double-check that everything went as planned\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Overview of credit default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Overview of loan statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an overview of the credit default by inspecting the loan statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].value_counts().plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will simplify the visual assessment by treating late payments equally. For modeling, I will only include 'Fully Paid' and 'Charged Off', to be able to compare apples with apples (both terminated). For now, I will leave in 'Current' and 'Late' for getting an overview of the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('Late (16-30 days)', 'Late', inplace=True)\n",
    "df.replace('Late (31-120 days)', 'Late', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['loan_status'].isin(['Late', 'Fully Paid', 'Charged Off', 'Current'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_d'] = pd.to_datetime(df['issue_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_issue = df.groupby(['issue_d', 'loan_status'])['id'].count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample on quarterly basis\n",
    "by_issue_resampled = by_issue.resample('Q').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot loan statuses over issue time\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "by_issue_resampled.drop('Current', axis=1).plot(ax=ax[0])\n",
    "ax[0].set_title('Number of loans by status')\n",
    "ax[0].set_xlabel('Issue date')\n",
    "ax[0].set_ylabel('Loans')\n",
    "by_issue_resampled.plot(ax=ax[1])\n",
    "ax[1].set_title('Number of loans by status incl. current')\n",
    "ax[1].set_xlabel('Issue date')\n",
    "ax[1].set_ylabel('Loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Default rate of terminated loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the historical default rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the dataset further into only terminated ones\n",
    "df = df[df['loan_status'].isin(['Charged Off', 'Fully Paid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the percentage distribution of loan status\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "df.groupby('loan_status')['loan_status'].count().apply(\n",
    "    lambda x: int(x/len(df_tmp)*100)).plot.bar(ax=ax, rot=0, title='Percentage of borrowers')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Percentage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Financial impact: losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the financial implication of the defaults, I will look at historical losses. For this I will use the loan and payment datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary dataframe with payment data to calculate losses\n",
    "df_tmp = pd.merge(payment, loan, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_defaults = df_tmp[df_tmp['loan_status'] == 'Charged Off']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_rate = payment_defaults['recoveries'] / (payment_defaults['loan_amnt']-payment_defaults['total_rec_prncp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_rate.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be something fishy about the recoveries data, so in the calcualtion of losses, we will just use the median recovery rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = payment_defaults['loan_amnt']-payment_defaults['total_rec_prncp'] * (1-recovery_rate.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of historical losses\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.distplot(losses, ax=ax)\n",
    "ax.set_title('Distribution of historical losses (estimated)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of historical losses has amounted to around 0.5bn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Set up the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the loan status as the target variable for the supervised learning setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only terminated loans, we are down to 252k rows (full dataset with ~800k), but it is more than enough for getting descent ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the loan status variable to 1s and 0s\n",
    "df['target'] = df['loan_status'].apply(lambda x: 1 if x == 'Charged Off' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the loan status feature as we will no longer need it\n",
    "df.drop(labels='loan_status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Preliminary data cleaning: remove low-value features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a handful of features in the data that will not be of any use for modeling. We remove the id columns, and the features with a lot of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id columns\n",
    "df.drop(labels=['Unnamed: 0_x', 'Unnamed: 0_y', 'member_id', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for NaNs\n",
    "nans = pd.DataFrame(index=df.columns, columns=['percentage NaNs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform NaN counts to percentages\n",
    "for feature in df.columns:\n",
    "    nans.loc[feature] = df[feature].isnull().sum()/(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage NaNs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>annual_inc_joint</th>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desc</th>\n",
       "      <td>0.650873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp_title</th>\n",
       "      <td>0.0550696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp_length</th>\n",
       "      <td>0.0391231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>5.13893e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pymnt_plan</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>installment</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded_amnt</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_rate</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_d</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application_type</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purpose</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_grade</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_acc</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_rec</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_acc</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_inc</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr_state</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip_code</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_ownership</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 percentage NaNs\n",
       "annual_inc_joint        0.999996\n",
       "desc                    0.650873\n",
       "emp_title              0.0550696\n",
       "emp_length             0.0391231\n",
       "title                5.13893e-05\n",
       "pymnt_plan                     0\n",
       "installment                    0\n",
       "funded_amnt_inv                0\n",
       "funded_amnt                    0\n",
       "int_rate                       0\n",
       "loan_amnt                      0\n",
       "issue_d                        0\n",
       "application_type               0\n",
       "purpose                        0\n",
       "grade                          0\n",
       "sub_grade                      0\n",
       "term                           0\n",
       "earliest_cr_line               0\n",
       "delinq_2yrs                    0\n",
       "total_acc                      0\n",
       "pub_rec                        0\n",
       "open_acc                       0\n",
       "annual_inc                     0\n",
       "addr_state                     0\n",
       "zip_code                       0\n",
       "home_ownership                 0\n",
       "target                         0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nans.sort_values(by='percentage NaNs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with more than 90% NaNs\n",
    "cols_to_remove = nans[nans['percentage NaNs'] > 0.90].index.tolist()\n",
    "df.drop(labels=cols_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Omit old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might argue that due to the abnormal credit markets during the financial crisis, the dynamics 2007-2010 were somewhat different. From the historical loan issue data, we also saw very low figures during the years prior to 2010, which might indicate the company to be in its infancy. Based on this, I choose to exclude datapoints prior to 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['issue_d'] > '01-01-2010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Split train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using stratified splitting between train and test data with respect to the target variable. This is to keep the imbalance equal between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into target (y) and features (X)\n",
    "X = df.drop(labels='target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the target imbalance is equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts().divide(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts().divide(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset indexes in train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape is', X_train.shape)\n",
    "print('y_train shape is', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a merged dataset for data visualization on the training data (to be able to visualize each feature wrt target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine features and target in train data\n",
    "train = pd.concat([y_train, X_train], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will go through each feature one by one to assess its usefulness and get to know the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, feature, threshold=2):\n",
    "    \"\"\"\n",
    "    Computes outliers based on standard deviation from the mean (set to 2 as default)\n",
    "    Returns the input dataframe without outliers\n",
    "    \"\"\"\n",
    "    mean = df[feature].mean()\n",
    "    std = df[feature].std()\n",
    "    upper = mean + 2*std\n",
    "    lower = mean - 2*std\n",
    "    outliers = df[(df[feature] > upper) | (df[feature] < lower)][feature]\n",
    "    return df[~df[feature].isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(df, feature, name, dtype, keep_outliers=True, size='normal'):\n",
    "    \"\"\"\n",
    "    Plots features where one gets to choose between 3 different options in the dtype parameter\n",
    "        If numerical: \n",
    "            left: distplot with kde \n",
    "            right: violinplot wrt target\n",
    "        If categorical: \n",
    "            left: countplot\n",
    "            right: barplot with central tendency wrt target\n",
    "        If other:\n",
    "            left: Distribution of defaulters wrt date feature\n",
    "            right: Distribution of non-defaulters wrt date feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create temporary dataframe that we can manipulate how we want\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp = df_tmp[df_tmp[feature].notnull()]\n",
    "    df_tmp['target_desc'] = df_tmp['target'].map({0: 'Non-default', 1: 'Default'})\n",
    "    \n",
    "    #Remove outliers to better visualize the feature\n",
    "    if keep_outliers==False:\n",
    "        df_tmp = remove_outliers(df_tmp, feature)\n",
    "    \n",
    "    if size == 'normal':\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(14,4))\n",
    "    elif size == 'big':\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(14,8))\n",
    "    \n",
    "    fig.suptitle(name)\n",
    "    \n",
    "    # Numerical features\n",
    "    if dtype == 'num':\n",
    "        \n",
    "        # left plot\n",
    "        sns.distplot(df_tmp[feature], kde=True, ax=ax[0])\n",
    "        ax[0].set_title('Distribution')\n",
    "        ax[0].set_xlabel(name)\n",
    "        ax[0].set_ylabel('Count')\n",
    "        \n",
    "        # right plot\n",
    "        sns.violinplot(x=feature, y='target_desc', data=df_tmp, ax=ax[1])\n",
    "        ax[1].set_title('Distribution of defaulters vs. non-defaulters')\n",
    "        ax[1].set_xlabel(name)\n",
    "        ax[1].set_ylabel('')\n",
    "    \n",
    "    elif dtype == 'cat':\n",
    "              \n",
    "        # left plot\n",
    "        sns.countplot(y=df_tmp[feature], order=sorted(df[feature].unique()), ax=ax[0])\n",
    "        ax[0].set_title('Count of loans')\n",
    "        ax[0].set_ylabel(name)\n",
    "        ax[0].set_xlabel('Count') \n",
    "            \n",
    "        # Right axis object\n",
    "        sns.barplot(y=feature, x='target', order=sorted(df[feature].unique()), data=df_tmp, ax=ax[1])\n",
    "        ax[1].set_title('Feature central tendency wrt default')\n",
    "        ax[1].set_ylabel('')\n",
    "        ax[1].set_xlabel('Central tendency of default')\n",
    "        \n",
    "    elif dtype == 'other':\n",
    "        \n",
    "        # left plot\n",
    "        df_tmp[df_tmp['target']==1][feature].hist(ax=ax[0], bins=20, color='darkred')\n",
    "        ax[0].set_title('Distribution for defaulters')\n",
    "        ax[0].set_xlabel(name)\n",
    "        ax[0].set_ylabel('Count')\n",
    "        \n",
    "        # right plot\n",
    "        df_tmp[df_tmp['target']==0][feature].hist(ax=ax[1], bins=20, color='darkblue')\n",
    "        ax[1].set_title('Distribution for non-defaulters')\n",
    "        ax[1].set_xlabel(name)\n",
    "        ax[1].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data on borrower/demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 State address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['addr_state'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='addr_state', name='US state of borrower address', dtype='cat', size='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be huge variances across states on default!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Annual income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annual_inc'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='annual_inc', name='Borrower Annual Income', dtype='num', keep_outliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-defaulters seem to have higher income, which is hardly surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Delinquency incidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['delinq_2yrs'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='delinq_2yrs', name='Delinquency Incidences', dtype='other', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be more deliquencies the last 2 years prior to the loan application for defaulters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Earliest report credit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['earliest_cr_line'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary variable of the feature in datetime type\n",
    "train['tmp'] = pd.to_datetime(train['earliest_cr_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='tmp', name='Earliest report credit line', dtype='other', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop temporary feature\n",
    "train.drop(labels='tmp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some variance in the data wrt target that could be useful for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 Employment length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment length is stored in text as categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['emp_length'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tmp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='emp_length', name='Employment length', dtype='cat', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some relationship between employment length and default. This variable will be processed to numericals prior to modeling though, to capture the continuous nature of the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6 Employment title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment title is a string variable that needs some extensive feature engineering to be useful. In interest of time, that will be left to a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['emp_title'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['emp_title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the train dataset by feature and aggregate on count and mean of target\n",
    "defaults_by_title = train.groupby('emp_title')['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults_by_title = defaults_by_title[defaults_by_title['count']>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 titles associated with the highest default rates\n",
    "defaults_by_title.sort_values(by='mean', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical blue-collar titles have the highest default mean. Interesting, but extensive feature engineering would be needed to make this variable more valuable in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.7 Home ownership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home ownership is a categorical variable. I merge the values 'none' and 'any' into the 'other' value to make it less fragmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['home_ownership'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tmp'] = train['home_ownership'].replace(to_replace=['NONE', 'ANY'], value='OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='tmp', name='Home ownership', dtype='cat', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(labels='tmp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowers with a mortgage seem to have lower default rates, whereas borrowers renting show the opposite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.8 Open credit lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['open_acc'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='open_acc', name='Open credit lines', dtype='num', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to see any noteworthy variance in open credit lines with respect to default. I will keep it as it might add value in combination with other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.9 Number of historic credit lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['total_acc'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='total_acc', name='Historic credit lines', dtype='num', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same story as with open credit lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.10 Number of derogatory public records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pub_rec'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pub_rec'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='pub_rec', name='Number of derogatory public records', dtype='other', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No apparent relationship, but we will keep it either way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.11 Zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip code will include the same data as address, but possibly on a more granular level. However, we only have the first 3 digits in the zip code. I decide to drop this and rather use address state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zip_code'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zip_code'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zip_code'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Loan data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Application type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No variance in this feature, hence we drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['application_type'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['application_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the wordcloud to look into this feature, but more extensive string manipulation and feature engineering would be needed to make use of this data. This is something one could explore at a later stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['desc'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = train['desc'][:100].dropna().apply(lambda x: x.strip()).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, desc in enumerate(wc):\n",
    "    print(desc[29:]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(df, feature):\n",
    "    wc = df[feature][:10000].dropna().apply(lambda x: x.strip()).copy()\n",
    "    for i, desc in enumerate(wc):\n",
    "        if desc[0:8] == 'Borrower':\n",
    "            wc[i] = desc[29:]\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = create_wordcloud(train, 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud().generate(' '.join(wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Total amount funded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is already included in the 'Loan amount' feature, as shown in the correlation below. It will thus we dropped prior to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['funded_amnt'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['funded_amnt'].corr(train['loan_amnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Amount committed by investors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is already included in the 'Total amount funded' feature, as shown in the correlation below. It will thus we dropped prior to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['funded_amnt_inv'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['funded_amnt_inv'].corr(train['loan_amnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Loan amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['loan_amnt'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='loan_amnt', name='Loan amount', dtype='num', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defaulters seem to have higher amounts on their loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.6 Loan grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is included, but on a more granular level in the 'Loan sub-grade' feature, thus we I use that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['grade'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.7 Loan sub-grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sub_grade'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='sub_grade', name='Loan sub-grade', dtype='cat', size='big', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly a significant relationship here, so the grading system seems to be working to some degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.8 Installment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['installment'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='installment', name='Installment', dtype='num', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defaulters seem to have slightly higher installments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.9 Interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['int_rate'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='int_rate', name='Interest rate', dtype='num', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As risky borrowers are charged higher interest rates, the relationship between the two is hardly surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.10 Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['purpose'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='purpose', name='Purpose', dtype='cat', size='big', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is definitely some interesting relationships here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.11 Payment plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature has almost no variance, hence we will drop it prior to modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pymnt_plan'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pymnt_plan'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.12 Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['term'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['term'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='term', name='Term', dtype='cat', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loan with longer terms seem to have a far higher probability of defaulting - hardly surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.13 Loan title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is already included in purpose, hence I will drop it prior to modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a feature for debt-to-income - a common metric for credit applications. It was originally in the payments data, but I assumed it to be real-time and thus couldn't be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Debt-to-income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Function to calcualte debt-to-income\n",
    "    \"\"\"\n",
    "    \n",
    "    df['dti'] = df['installment'] / (df['annual_inc'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature(train, feature='dti', name='Debt-to-income', dtype='num', keep_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defaulters seem to have a higher dti "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_to_num(date_col):\n",
    "    \"\"\"\n",
    "    Function to convert date variables to number format\n",
    "    \"\"\" \n",
    "    \n",
    "    return date_col.apply(lambda x:(parse(x)-datetime(1900,1,1)).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, ohe=False):\n",
    "    \"\"\"\n",
    "    Function for data pre-processing\n",
    "    \n",
    "    The parameter ohe lets the user choose whether to do one-hot-encoding or transform those variables to categoricals\n",
    "    \n",
    "    returns processed DataFrame \n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    feature_engineering(df_new)\n",
    "    \n",
    "    # Columns to drop\n",
    "    cols_to_drop = ['emp_title', 'zip_code', 'application_type', 'desc', 'funded_amnt', 'funded_amnt_inv', 'grade', \n",
    "                    'pymnt_plan', 'title', 'issue_d',]\n",
    "    \n",
    "    # Drop columns\n",
    "    df_new.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    # Transform date column to int\n",
    "    df_new['earliest_cr_line'] = convert_date_to_num(df_new['earliest_cr_line'])\n",
    "    \n",
    "    # Clean employment length feature\n",
    "    df_new['emp_length'].replace('10+ years', '10 years', inplace=True)\n",
    "    df_new['emp_length'].replace('< 1 year', '0 years', inplace=True)\n",
    "    df_new['emp_length'].replace('n/a', np.nan, inplace=True)\n",
    "    df_new['emp_length'] = df_new['emp_length'].apply(lambda x: x if pd.isnull(x) else np.int8(x.split()[0]))\n",
    "    \n",
    "    # Clean home ownership feature\n",
    "    df_new['home_ownership'].replace(to_replace=['NONE', 'ANY'], value='OTHER', inplace=True)\n",
    "    \n",
    "    cat_cols = df_new.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Performns ohe or transforming to categoricals\n",
    "    if ohe:\n",
    "        dummies = pd.get_dummies(df_new[cat_cols])\n",
    "        df_new = df_new.drop(cat_cols, axis=1).join(dummies)\n",
    "    else:  \n",
    "        for col in cat_cols:\n",
    "            df_new[col] = df_new[col].astype('category')\n",
    "        \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame includes ohe vars and will be used for models that need ohe \n",
    "X_train_processed = data_preprocessing(X_train, ohe=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame has categorical vars as categorical dtype and will be used for models that support that\n",
    "X_train_processed2 = data_preprocessing(X_train, ohe=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 202376 entries, 0 to 202375\n",
      "Data columns (total 16 columns):\n",
      "emp_length          194476 non-null float64\n",
      "home_ownership      202376 non-null category\n",
      "addr_state          202376 non-null category\n",
      "annual_inc          202376 non-null float64\n",
      "open_acc            202376 non-null float64\n",
      "pub_rec             202376 non-null float64\n",
      "total_acc           202376 non-null float64\n",
      "delinq_2yrs         202376 non-null float64\n",
      "earliest_cr_line    202376 non-null int64\n",
      "term                202376 non-null category\n",
      "sub_grade           202376 non-null category\n",
      "purpose             202376 non-null category\n",
      "loan_amnt           202376 non-null int64\n",
      "int_rate            202376 non-null float64\n",
      "installment         202376 non-null float64\n",
      "dti                 202376 non-null float64\n",
      "dtypes: category(5), float64(9), int64(2)\n",
      "memory usage: 18.0 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_processed2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_processed (with ohe) shape is (202376, 117)\n",
      "X_train_processed2 (with categoricals) shape is (202376, 16)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_processed (with ohe) shape is', X_train_processed.shape)\n",
    "print('X_train_processed2 (with categoricals) shape is', X_train_processed2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modeling process is done as following: <br>\n",
    "1) Spot-check 3 different algos using GridSearchCV <br>\n",
    "2) Optimize hyperparameters for the best performing algo using HypOpt on a sample of the training data <br>\n",
    "3) See results on best model on full training data (still using CV) <br>\n",
    "4) Look at feature importances <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Spot-check 3 different algorithms using grid search cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Logistic regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with both imputation and standard scaling\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=seed, warm_start=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters grid\n",
    "logreg_param_grid  = {\n",
    "    'model__alpha': [10**-5, 10**-2, 10**1],\n",
    "    'model__penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV object for the logistic regression\n",
    "GridSearchCV_logreg = GridSearchCV(estimator=logreg_pipeline, param_grid=logreg_param_grid,\n",
    "                                   scoring='roc_auc', n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, \n",
    "                                   verbose=1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to pre-processed training data (with ohe)\n",
    "GridSearchCV_logreg.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best roc-auc score amongst the iterations\n",
    "GridSearchCV_logreg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "GridSearchCV_logreg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for Random Forest Classifier. Scaling is not needed for tree-based models\n",
    "rfc_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('model', RandomForestClassifier(n_jobs=-1, random_state=seed))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to limit the RF grid because it's very computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_param_grid  = {\n",
    "    'model__n_estimators': [100],\n",
    "    #'model__max_features': ['auto'],\n",
    "    #'model__max_depth': [None, 3]\n",
    "}\n",
    "pprint(rfc_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV object for RF\n",
    "GridSearchCV_rfc = GridSearchCV(estimator=rfc_pipeline, param_grid=rfc_param_grid,\n",
    "                                scoring='roc_auc', n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit to pre-processed training data (with ohe)\n",
    "GridSearchCV_rfc.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV_rfc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Gradient Boosting Classifier: LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the LightGBM algo for Gradient Boosting, both due to its performance and speed. The LightGBM does not need ohe categorical variables, thus we will use the X_train_preprocessed2 dataframe for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(seed = seed, boosting_type='gbdt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid for LGBM\n",
    "lgb_param_grid  = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchVB object for LGBM\n",
    "GridSearchCV_lgb = GridSearchCV(estimator=lgb_model, param_grid=lgb_param_grid,\n",
    "                                   scoring='roc_auc', n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, \n",
    "                                   verbose=1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   40.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, seed=42,\n",
       "        silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "        subsample_freq=0),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'learning_rate': [0.001, 0.01, 0.1], 'subsample': [0.8, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV_lgb.fit(X_train_processed2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV_lgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV_lgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 Compare algorithms in plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the algorithms in a plot, I will extract the scores from each cv with the best hyperparameters in the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to match the keys of interst in the GridSearchCV object\n",
    "regex = re.compile('split._test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('SGD Logistic Regression', GridSearchCV_logreg),\n",
    "            ('Random Forest Classifier', GridSearchCV_rfc),\n",
    "            ('LightGBM', GridSearchCV_lgb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame for results\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each (name, GridSearchCV object) tuple in in the models list \n",
    "# and extract the results in the cv with the best hyperparameters\n",
    "for name, grid_cv in models:\n",
    "    ix = grid_cv.best_index_\n",
    "    for key in grid_cv.cv_results_:\n",
    "        if re.match(regex, key):\n",
    "            results.loc[key[5], name] = grid_cv.cv_results_[key][ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack the dataset for plotting\n",
    "results = results.unstack().reset_index(level=0)\n",
    "results.columns = ['Model', 'cv results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "sns.swarmplot(x='Model', y='cv results', data=results, ax=ax)\n",
    "ax.set_ylabel('Area under the AUC curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the LightGBM Gradient Boosting algorithm performs consistently better, and always > 0.70 which is descent.\n",
    "\n",
    "I will progress with the LightGBM model in the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Bayesian Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use Bayesian Hyperparameter Optimization with the Hyperopt library to find optimal hyperparameters for the LightGBM model. Due to its computational requirement, I will sample the training dataset. As this is merely on the prototyping stage, this seems reasonable. At a later stage, one might want to search further with more iterations and the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Create sample dataset to limit computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "max_evals = 1000\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X and y train datasets and sample 20k rows\n",
    "train_bayes = pd.concat([y_train, X_train_processed2], axis=1)\n",
    "train_bayes = train_bayes.sample(n= 20000, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training dataset for hyperparameter optimization in X and y\n",
    "y_train_bayes = np.array(train_bayes['target'])\n",
    "X_train_bayes = train_bayes.drop('target', axis=1)\n",
    "print('Train features shape: ', X_train_bayes.shape)\n",
    "print('Train labels shape: ', y_train_bayes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset required for LGB cv method\n",
    "train_set = lgb.Dataset(X_train_bayes, label = y_train_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Set up components: objective fuction, domain and optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian hyperparameter optimization requires 3 components: <br>\n",
    "1) The objective function to optimize - in our situation the CV auc score for the LightGBM model <br>\n",
    "2) Bayesian domain - equivalent to the hyperparameter grid, but one needs to define probability distributions to sample from <br>\n",
    "3) Optmization algorithm - I will use the TPE (Tree Parzen Estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, seed=seed, n_folds=n_folds):\n",
    "    \"\"\"\n",
    "    Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n",
    "    Writes a new line to 'outfile' on every iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Using early stopping to find number of trees trained\n",
    "    if 'n_estimators' in hyperparameters:\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = seed)\n",
    "\n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = cv_results['auc-mean'][-1]\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = len(cv_results['auc-mean'])\n",
    "    \n",
    "    # Add the number of estimators to the hyperparameters\n",
    "    hyperparameters['n_estimators'] = n_estimators\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(OUT_FILE, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n",
    "    of_connection.close()\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bayesian Domain\n",
    "space = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'subsample': hp.uniform('gdbt_subsample', 0.5, 1), \n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimization algorithm\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record results\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file and open a connection\n",
    "OUT_FILE = 'bayesian_hypopt_test.csv'\n",
    "of_connection = open(OUT_FILE, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Write column names\n",
    "headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Run the hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global variable\n",
    "global ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials, max_evals = max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in results recorded in the csv file \n",
    "results = pd.read_csv(OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print results sorted by score\n",
    "results.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Plot score results over iterations in the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(results, name):\n",
    "    \"\"\"Evaluate model on test data using hyperparameters in results\n",
    "       Return dataframe of hyperparameters\"\"\"\n",
    "    \n",
    "    new_results = results.copy()\n",
    "    # String to dictionary\n",
    "    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n",
    "    \n",
    "    # Sort with best values on top\n",
    "    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Print out cross validation high score\n",
    "    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.\n",
    "          format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))\n",
    "    \n",
    "    # Use best hyperparameters to create a model\n",
    "    hyperparameters = new_results.loc[0, 'hyperparameters']\n",
    "    model = lgb.LGBMClassifier(**hyperparameters)\n",
    "    \n",
    "    # Train and make predictions\n",
    "    model.fit(X_train_bayes, y_train_bayes)\n",
    "    \n",
    "    # Create dataframe of hyperparameters\n",
    "    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n",
    "\n",
    "    # Iterate through each set of hyperparameters that were evaluated\n",
    "    for i, hyp in enumerate(new_results['hyperparameters']):\n",
    "        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n",
    "                               ignore_index = True)\n",
    "        \n",
    "    # Put the iteration and score in the hyperparameter dataframe\n",
    "    hyp_df['iteration'] = new_results['iteration']\n",
    "    hyp_df['score'] = new_results['score']\n",
    "    \n",
    "    return hyp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params = evaluate(results, name = 'Bayesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of just scores\n",
    "scores = pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration']})\n",
    "\n",
    "scores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\n",
    "scores['iteration'] = scores['iteration'].astype(np.int32)\n",
    "\n",
    "best_bayes_params = bayes_params.iloc[bayes_params['score'].idxmax(), :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of scores over the course of searching\n",
    "sns.lmplot('iteration', 'ROC AUC', data = scores, size = 3.5, aspect=2)\n",
    "plt.scatter(best_bayes_params['iteration'], best_bayes_params['score'], marker = '*', s = 400, c = 'darkblue', edgecolor = 'k')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title(\"Validation ROC AUC versus Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there is a positive trend in the score outcome with respect to number of iterations - something we would expect for bayesian hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Results of best model on full train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so far only shown results on a sample of the train data for the best model. It's time to test it on the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset required for LGB cv method\n",
    "full_train_set = lgb.Dataset(X_train_processed2, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast variables to the appropriate data type\n",
    "best['subsample_for_bin'] = int(best['subsample_for_bin'])\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['min_child_samples'] = int(best['min_child_samples'])\n",
    "best['is_unbalance'] = bool(best['is_unbalance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with optimal hyperparameters to the full training data\n",
    "cv_result = lgb.cv(best, full_train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of boosting iterations used in the fitted model\n",
    "len(cv_result['auc-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the auc score for the full dataset\n",
    "cv_result['auc-mean'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I note that the roc-auc score with the optimal hyperparameters found in the bayesian optimization is actually lower than the spot-checked. The reason is very likely to be that we used sampled training data for the optimization. To enhance performance, one would need more computational power and run the hyperparameter optimization on the full dataset to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Feature importances with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for importance and labels and indeces to sort them in the next step\n",
    "importances = best_lgb.feature_importances_\n",
    "feat_labels = X_train_processed2.columns\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the top_n features\n",
    "fig, ax = plt.subplots(figsize=(10,6))    \n",
    "top_n = 10\n",
    "ax.set_title('Feature Importances')\n",
    "ax.barh(range(top_n), importances[indices[0:top_n]], align = 'center')\n",
    "plt.yticks(range(top_n), feat_labels[indices[0:top_n]], rotation=0)\n",
    "plt.ylim([-1, top_n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Results on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Pre-process test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the same data pre-processing function that we used for the train data. One-hot-encoding is not needed as the LightGBM handles categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed = data_preprocessing(X_test, ohe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    \"\"\"\n",
    "    \n",
    "    return [1 if y >= t else 0 for y in y_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_by_threshold(y_scores, y_actual, t):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix by threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # Run function to adjust class prediction by threshold\n",
    "    pred_adj = adjusted_classes(y_scores, t)\n",
    "    \n",
    "    return confusion_matrix(y_actual, pred_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_scores, y_actual, classes,\n",
    "                          t, normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    cm = cm_by_threshold(y_scores, y_actual, t=t)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prediction scores for the test data\n",
    "y_test_scores = best_lgb.predict_proba(X_test_processed)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test_scores, y_test, ['Non-default', 'Default'], t=0.70, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 ROC curve and ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, roc_auc):\n",
    "    \"\"\"\n",
    "    Function to plot the roc curve\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the roc_auc for the test data\n",
    "roc_auc = roc_auc_score(y_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Area Under the ROC curve is', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note that the auc is actually higher for the test data than for the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result on the testing data looks very promising, with a descent shaped ROC curve.\n",
    "\n",
    "One would need to carefully select the threshold based on the preference on TPR vs FPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Comparison with sub-grade model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the model's results with one that might be in place today - based on the subgrades. By mapping a probability linearly along the subgrades (giving 0% pd on A1 and 50% pd on G5), we might get something that is similar to what is being used today. By building a confusion matrix and retrieving the roc-auc for the sub-grade model, we can judge how much value the machine learning model adds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sorted list of subgrades\n",
    "sorted_subgrades = sorted(X_test['sub_grade'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for subgrades with probability of default as a linear function from 0 to 50%\n",
    "subgrades_df = pd.DataFrame({'sub_grade': sorted_subgrades, \n",
    "                             'pd': np.linspace(0, 0.5, len(sorted_subgrades))}).set_index('sub_grade', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map in default probabilities in test dataset\n",
    "y_test_scores_subgrades = X_test['sub_grade'].map(lambda x: subgrades_df.loc[x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix with a threshold that returns a similar number of TP as our ML model\n",
    "plot_confusion_matrix(y_test_scores_subgrades, y_test, ['Non-default', 'Default'], t=0.17, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tweeking the threshold of the subgrade model to match the ML model's number of TPs, one can compare the performance by looking at the confusion matrices. The ML model clearly outperforms the subgrade model with 201 less FNs and 1307 less FPs, clearly demonstrating the ML model’s added business value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve roc_auc score for the sub-grade model\n",
    "roc_auc_subgrades = roc_auc_score(y_test, y_test_scores_subgrades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Area Under the ROC curve is', roc_auc_subgrades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_scores_subgrades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, roc_auc_subgrades)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
